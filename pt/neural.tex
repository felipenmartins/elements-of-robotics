% !TeX root = er.tex

\chapter{Redes Neurais}\label{ch.neural}

O capítulo~\ref{ch.reactive} descreveu comportamentos reativos inspirados no trabalho de Valentino Braitenberg. O controle de veículos Braitenberg simples é muito semelhante ao controle de um organismo vivo por sua rede \emph{biológica neural}. Este termo se refere ao sistema nervoso de um organismo vivo, incluindo seu cérebro e os nervos que transmitem sinais através do corpo. Os modelos computadorizados de redes neurais são um tópico ativo de pesquisa em inteligência artificial. \emph{Artificial neural networks (ANNs)} permitem implementar um comportamento complexo usando um grande número de componentes abstratos relativamente simples que são modelados em neurônios, os componentes das redes neurais biológicas. Este capítulo apresenta o uso das ANNs para controlar o comportamento dos robôs.

Seguindo uma breve visão geral do sistema nervoso biológico na Sect.~\ref{s.bio-nn}, Sect.~ref{s.ann} define o modelo de ANN e Sect.~\ref{s.braitenberg-ann} mostra como ele pode ser usado para implementar o comportamento de um veículo Braitenberg. A seção ~\ref{s.ann-topology} apresenta diferentes topologias de rede. A característica mais importante das ANNs é sua capacidade de aprendizagem que lhes permite adaptar seu comportamento. A seção~\ref{s.ann-learning} apresenta uma visão geral da aprendizagem nas ANNs usando a regra Hebbian.

\section{O sistema neural biológico}\label{s.bio-nn}

O sistema nervoso dos organismos vivos consiste em células chamadas \emph{neurônios} que processam e transmitem informações dentro do corpo. Cada neurônio realiza uma operação simples, mas a combinação destas operações leva a um comportamento complexo. A maioria dos neurônios está concentrada no cérebro, mas outros formam os nervos que transmitem sinais de e para o cérebro. Em \emph{vertebrados} como nós, muitos neurônios estão concentrados na medula espinhal que transmitem sinais de forma eficiente por todo o corpo. Há um número imenso de neurônios em um ser vivo: o cérebro humano tem cerca de US\$ 100 bilhões de neurônios, enquanto até mesmo um cérebro de rato tem cerca de US\$ 71 milhões de neurônios \cite{herculano2009human}. 

A figura~\ref{fig.neuron} mostra a estrutura de um neurônio. Ele tem um corpo principal com um núcleo e uma fibra longa chamada de \emph{axônio}, que permite a conexão de um neurônio com outro. O corpo de um neurônio tem projeções chamadas \emph{dendritos}. Axônios de outros neurônios se conectam com os dendritos através de \emph{sinapses}. Os neurônios funcionam através de processos bioquímicos que são bem compreendidos, mas podemos abstrair esses processos em \emph{pulsos} que viajam de um neurônio para outro. Os pulsos de entrada são recebidos através das sinapses para os dendritos e deles para o corpo do neurônio, que processa os pulsos e por sua vez transmite um pulso de saída através do axônio. O processamento no corpo de um neurônio pode ser abstraído como uma função dos pulsos de entrada para um pulso de saída, e as sinapses regulam a transmissão dos sinais. As sinapses são adaptáveis e são o elemento primário que torna possível a memória e a aprendizagem.

\begin{figure}
\begin{center}
\includegraphics[width=.8\textwidth,keepaspectratio]{neuron}
\end{center}
\caption{Estrutura de um neurônio.\\https://commons.wikimedia.org/wiki/File:Neuron.svg by Dhp1080 [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0) or GFDL (https://en.wikipedia.org/wiki/en:GNU\_Free\_Documentation\_License)], via Wikimedia Commons.
}\label{fig.neuron}
\end{figure}

\section{O modelo de rede neural artificial}\label{s.ann}

Um neurônio artificial é um modelo matemático de um neurônio biológico (Figs.~\ref{fig.artificial-neuron-1}--\ref{fig.artificial-neuron-2}; veja a Tabela~\ref{tab.ann-symbols} para uma lista dos símbolos que aparecem nos diagramas ANN). O corpo do neurônio é um nó que executa duas funções: ele calcula a soma dos sinais de entrada ponderados e aplica uma função de saída à soma. Os sinais de entrada são multiplicados por pesos antes que as funções de soma e saída sejam aplicadas; isto modela a sinapse. A função de saída é normalmente não linear; exemplos são: (1) conversão da saída do neurônio para um conjunto de valores discretos (acender uma luz \p{on} ou \p{off}); (2) limitação da faixa dos valores de saída (a potência do motor pode estar entre $-100$ e $100$; (3) normalização da faixa dos valores de saída (o volume de um som está entre $0$ (mudo) e $1$ (máximo).

\begin{figure}[t]
% Artificial neurons
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}[node distance=1.2cm and 2.2cm, on grid]
\pic { nnnode={nn} };
\node (output) [right=of nn] {};
\node (input) [left=of nn] {};
\node [below left=of nn] {};
\draw[->] (nn.east) -- (output.west) node[xshift=5pt] {$y_1$};
\draw[->] (input.east) node [xshift=-6pt] {$x_1$} -- node[w]  { $w_1$ } (nn.west);
\draw (input) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
\end{tikzpicture}
\caption{ANN: um neurônio com uma entrada}\label{fig.artificial-neuron-1}
\end{minipage}
\hspace{\fill}
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}[node distance=1.2cm and 2.2cm, on grid]
\pic { nnnode={nn} };
\node (output) [right=of nn] {};
\node (input1) [above left=of nn] {};
\node (input2) [below left=of nn] {};
\draw[->] (nn.east) -- (output.west) node[xshift=5pt] {$y_1$};
\draw[->] (input1.east) node [xshift=-6pt] {$x_1$} -- node[w]  { $w_1$ } (nn.north west);
\draw[->] (input2.east) node [xshift=-6pt] {$x_2$} -- node[w]  { $w_2$ } (nn.south west);
\draw (input1) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
\draw (input2) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
\end{tikzpicture}
\caption{ANN: um neurônio com duas entradas}\label{fig.artificial-neuron-2}
\end{minipage}
\end{figure}

%\begin{figure}[t]
%% Artificial neurons
%\subfigures
%\begin{minipage}{\textwidth}
%\leftfigure{
%\begin{tikzpicture}[node distance=1.2cm and 2.2cm, on grid]
%\pic { nnnode={nn} };
%\node (output) [right=of nn] {};
%\node (input) [left=of nn] {};
%\node [below left=of nn] {};
%\draw[->] (nn.east) -- (output.west) node[xshift=5pt] {$y_1$};
%\draw[->] (input.east) node [xshift=-6pt] {$x_1$} -- node[w]  { $w_1$ } (nn.west);
%\draw (input) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%\end{tikzpicture}
%}
%\hspace{\fill}
%\rightfigure{
%\begin{tikzpicture}[node distance=1.2cm and 2.2cm, on grid]
%\pic { nnnode={nn} };
%\node (output) [right=of nn] {};
%\node (input1) [above left=of nn] {};
%\node (input2) [below left=of nn] {};
%\draw[->] (nn.east) -- (output.west) node[xshift=5pt] {$y_1$};
%\draw[->] (input1.east) node [xshift=-6pt] {$x_1$} -- node[w]  { $w_1$ } (nn.north west);
%\draw[->] (input2.east) node [xshift=-6pt] {$x_2$} -- node[w]  { $w_2$ } (nn.south west);
%\draw (input1) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%\draw (input2) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%\end{tikzpicture}
%}
%\leftcaption{ANN: one neuron with one input}\label{fig.artificial-neuron-1}
%\rightcaption{ANN: one neuron with two inputs}\label{fig.artificial-neuron-2}
%\end{minipage}
%\end{figure}

\begin{table}
\caption{Símbolos usados nos diagramas ANN}
\label{tab.ann-symbols}
\begin{tabular}{p{2cm}p{4cm}}
\hline\noalign{\smallskip}
Símbolo & Significado \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$f$ & Função de saída de neurônios\\
$+$ & Soma das entradas\\
$x_i$ & Entradas\\
$y_i$ & Saídas\\
$w_{i}$ & Pesos para as entradas\\
$1$ & Entrada constante de valor $1$\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

Os neurônios artificiais são modelos analógicos, ou seja, as entradas, saídas, pesos e funções podem ser números de ponto flutuante. Aqui começamos com uma atividade irrealista que demonstra como os neurônios artificiais funcionam dentro do contexto familiar dos portões lógicos digitais.

A Figura~\ref{fig.not-gate} mostra um neurônio artificial com duas entradas, $x_1$ e $1$, e uma saída $y$. O significado da entrada $1$ é que a entrada não está conectada a um sensor externo, mas, em vez disso, retorna um valor constante de $1$. O valor de entrada de $x_1$ é assumido como sendo $0$ ou $1$. A função $f$ é:
\[
\begin{array}{ll}
f(x) = 0 & \;\;\;\textrm{if} \;\; x < 0\\
f(x) = 1 & \;\;\;\textrm{if} \;\; x \geq 0\,.
\end{array}
\]
Mostrar que com os pesos indicados o neurônio implementa a porta lógica para \p{not}.

\begin{framed}
\act{Neurônios artificiais para portões lógicos}{logic-gates}
\begin{itemize}
\item O neurônio artificial em Fig.~\ref{fig.and-or-gate} tem uma entrada adicional de $x_2$. Atribuir pesos $w_0$, $w_1$, $w_2$ para que $y$ seja $1$ somente se os valores de $x_1$ ou $x_2$ (ou ambos) forem $1$. Isto implementa a porta lógica para  \p{or}.
\item Atribuir pesos $w_0$, $w_1$, $w_2$ para que $y$ seja $1$ somente se os valores de $x_1$ e $x_2$ forem ambos $1$. Isto implementa a porta lógica para \p{and}.
\item Implementar os neurônios artificiais para portões lógicos em seu robô. Use dois sensores, um para $x_1$ e outro para $x_2$. Use a saída $y$ (mapeada por $f$, se necessário) para que uma saída de $0$ dê um comportamento e uma saída de $1$ outro comportamento, como ligar ou desligar uma luz, ou iniciar e parar o robô.
\end{itemize}
\end{framed}

\begin{figure}
% Logic gates
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}%
[node distance=1.5cm and 2cm, nn/.style={circle,draw,minimum size=16pt}]
% Not
% Inputs
\node (i1) {};
\node (i2) [below=of i1] {};
\node [below=of i2] {};  % Dummy node to align on neuron
\foreach \i in {i1,i2}
  \draw (\i) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
% Neuron
\pic [right=of i2] { nnnode={neuron} };
% Output
\node (output) [node distance=1.5cm and 1cm,right=of neuron] {};
\draw[->] (neuron) -- (output) node {$y$};
% Arrows to neuron
\draw[->] (i1) node [xshift=-5pt] {$1$} -- node[fill=white] {$10$} (neuron);
\draw[->] (i2) node [xshift=-6pt] {$x_1$} -- node[fill=white] {$-20$} (neuron);
\end{tikzpicture}
\caption{Neurônio artificial para o portão \textsf{not} do teto}\label{fig.not-gate}
\end{minipage}
\hspace{\fill}
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}%
[node distance=1.5cm and 2cm, nn/.style={circle,draw,minimum size=16pt}]
% And and Or
% Inputs
\node(i1) {};
\node (i2) [below=of i1] {};
\node (i3) [below=of i2] {};
\foreach \i in {i1,i2,i3}
  \draw (\i) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
% Neuron
\pic [right=of i2] { nnnode={neuron} };
% Output
\node (output) [node distance=1.5cm and 1cm,right=of neuron] {};
\draw[->] (neuron) -- (output) node {$y$};
% Arrows to neuron
\draw[->] (i1) node [xshift=-6pt] {$1$} -- node[fill=white] {$w_0$} (neuron);
\draw[->] (i2) node [xshift=-6pt] {$x_1$} -- node[fill=white] {$w_1$} (neuron);
\draw[->] (i3) node [xshift=-6pt] {$x_2$} -- node[fill=white] {$w_2$} (neuron);
\end{tikzpicture}
\caption{Neurônio artificial para os portões \textsf{and} e \textsf{or}}\label{fig.and-or-gate}
\end{minipage}
\end{figure}

%\begin{figure}
%% Logic gates
%\subfigures
%\begin{minipage}{\textwidth}
%\leftfigure{
%\begin{tikzpicture}%
%[node distance=1.5cm and 2cm, nn/.style={circle,draw,minimum size=16pt}]
%% Not
%% Inputs
%\node (i1) {};
%\node (i2) [below=of i1] {};
%\node [below=of i2] {};  % Dummy node to align on neuron
%\foreach \i in {i1,i2}
%  \draw (\i) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%% Neuron
%\pic [right=of i2] { nnnode={neuron} };
%% Output
%\node (output) [node distance=1.5cm and 1cm,right=of neuron] {};
%\draw[->] (neuron) -- (output) node {$y$};
%% Arrows to neuron
%\draw[->] (i1) node [xshift=-5pt] {$1$} -- node[fill=white] {$10$} (neuron);
%\draw[->] (i2) node [xshift=-6pt] {$x_1$} -- node[fill=white] {$-20$} (neuron);
%\end{tikzpicture}
%}
%\hspace{\fill}
%\rightfigure{
%\begin{tikzpicture}%
%[node distance=1.5cm and 2cm, nn/.style={circle,draw,minimum size=16pt}]
%% And and Or
%% Inputs
%\node(i1) {};
%\node (i2) [below=of i1] {};
%\node (i3) [below=of i2] {};
%\foreach \i in {i1,i2,i3}
%  \draw (\i) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%% Neuron
%\pic [right=of i2] { nnnode={neuron} };
%% Output
%\node (output) [node distance=1.5cm and 1cm,right=of neuron] {};
%\draw[->] (neuron) -- (output) node {$y$};
%% Arrows to neuron
%\draw[->] (i1) node [xshift=-6pt] {$1$} -- node[fill=white] {$w_0$} (neuron);
%\draw[->] (i2) node [xshift=-6pt] {$x_1$} -- node[fill=white] {$w_1$} (neuron);
%\draw[->] (i3) node [xshift=-6pt] {$x_2$} -- node[fill=white] {$w_2$} (neuron);
%\end{tikzpicture}
%}
%\leftcaption{Artificial neuron for the \textsf{not} gate}\label{fig.not-gate}
%\rightcaption{Artificial neuron for the \textsf{and} and \textsf{or} gates}\label{fig.and-or-gate}
%\end{minipage}
%\end{figure}

\noindent{}Tatividade seguinte explora o processamento analógico em um neurônio artificial.

\begin{framed}
\act{Neurônios artificiais analógicos}{analog neuron}
\begin{itemize}
\item Implementar o neurônio artificial mostrado na Fig.~\ref{fig.artificial-neuron-1} para que ele demonstre o seguinte comportamento. A entrada para o neurônio será a leitura de um sensor de proximidade na frente do robô. A saída será uma ou ambas das seguintes: (1) a intensidade de uma luz no robô ou o volume do som de um alto-falante no robô; (2) a potência do motor aplicada aos motores esquerdo e direito para que o robô se retraia de um objeto detectado pelo sensor.
\item O valor de saída será proporcional ao valor de entrada: quanto mais próximo do objeto, maior a intensidade (ou volume); quanto mais próximo do objeto, mais rápido o robô se retira do objeto.
\item Modificar a implementação para que haja duas entradas a partir de dois sensores de proximidade (Fig.~\ref{fig.artificial-neuron-2}). Dar valores diferentes para os dois pesos $w_1$, $w_2$ e mostrar que o sensor conectado à entrada com o maior peso tem mais efeito sobre a saída.
\end{itemize}
\end{framed}


\section{Implementando um veículo Braintenberg com um ANN}\label{s.braitenberg-ann}

A figura~\ref{fig.nn-avoidance} mostra um robô inspirado em um veículo Braitenberg cujo comportamento é implementado utilizando uma simples rede neural. Descrevemos a ANN em detalhes e depois damos várias atividades que lhe pedem para projetar e implementar o algoritmo.

\begin{quote}
\normalsize
\noindent\textbf{Especificação (evitar obstáculos):}\\
O robô tem três sensores voltados para frente.
\begin{itemize}
\item O robô avança, a menos que detecte um obstáculo.
\item Se o obstáculo for detectado pelo sensor central, o robô se move lentamente para trás.
\item Se o obstáculo for detectado pelo sensor esquerdo, o robô vira à direita.
\item Se o obstáculo for detectado pelo sensor direito, o robô vira à esquerda.
\end{itemize}
\end{quote}

\begin{figure}
\begin{center}
\begin{tikzpicture}
% Draw big robot
\draw (-2.2cm,-3.5cm) to [rounded corners] (5cm,-3.5cm) to%
[rounded corners, bend right=45] (5cm,3.5cm) to (-2.2cm,3.5cm) to cycle;
\fill (-1.2cm,-3.5cm) rectangle +(2.5cm, -.4cm);
\fill (-1.2cm,3.5cm) rectangle +(2.5cm, .4cm);
% Draw two sum nodes and arrows to wheels
\node[draw,circle split] at (0,14mm) (left) {$f$ \nodepart{lower} $+$};
\node[draw,circle split] at (0,-14mm) (right) {$+$ \nodepart{lower} $f$};
\draw[->,thick] (left) -- node[fill=white] {$y_1$} (0,34mm);
\draw[->,thick] (right) -- node[fill=white] {$y_2$} (0,-34mm);
% Left sensor and arrows
\draw[red] (5, 3) arc[start angle=135, end angle=315, radius=.2cm];
\node at (5.2,2.9) {$x_1$};
\draw[->,thick,red] (4.95,2.8) -- node[w] {$w_{\textit{\scriptsize pos}}$} (18pt,36pt);
\draw[->,thick,red] (5,2.7) -- node[w,near start,xshift=6pt] {$-w_{\textit{\scriptsize neg}}$} (right.north east);
% Center sensor and arrows
\draw[green!60!black] (6, .2) arc[start angle=90, end angle=270, radius=.2cm];
\node at (6.2,0) {$x_2$};
\draw[->,thick,green!60!black] (5.8,.1) -- node[w] {$-w_{\textit{\scriptsize back}}$} (16pt,32pt);
\draw[->,thick,green!60!black] (5.8,-.1) -- node[w] {$-w_{\textit{\scriptsize back}}$} (16pt,-32pt);
% Right sensor and arrows
\draw[blue] (5.3, -2.8) arc[start angle=45, end angle=225, radius=.2cm];
\node at (5.2,-2.95) {$x_3$};
\draw[->,thick,blue] (5,-2.8) -- node[w,near start,xshift=6pt] {{$-w_{\textit{\scriptsize neg}}$}} (left.south east);
\draw[->,thick,blue] (4.95,-2.9) -- node[w] {$w_{\textit{\scriptsize pos}}$} (18pt,-36pt);
% Constant weight
\node[fill,circle] (power) at (-2.0,0) {};
\node at (-1.7,0) {$1$};
\draw[->,thick] (power.north east) -- node[fill=white] {$w_{\textit{\scriptsize fwd}}$} (-17pt,34pt);
\draw[->,thick] (power.south east) -- node[fill=white] {$w_{\textit{\scriptsize fwd}}$} (-17pt,-34pt);
\end{tikzpicture}
\end{center}
\caption{Rede neural para evitar obstáculos}\label{fig.nn-avoidance}
\end{figure}

\noindent{}A figura~\ref{fig.nn-avoidance} mostra os dois neurônios cujas saídas controlam a potência enviada para os motores das rodas do robô. A tabela~\ref{tab.obstacle-avoidance} lista os símbolos usados na figura.

\begin{table}
\caption{Símbolos em Fig.~\ref{fig.nn-avoidance} além dos da tabela~\ref{tab.ann-symbols}}
\label{tab.obstacle-avoidance}
\begin{tabular}{p{2cm}p{5.5cm}}
\hline\noalign{\smallskip}
Símbolo & Significado \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$w_{\textit{\scriptsize fwd}}$ & Peso para o movimento de avanço\\
$w_{\textit{\scriptsize back}}$ & Peso para o movimento para trás\\
$w_{\textit{\scriptsize pos}}$ & Peso para rotação positiva da roda\\
$w_{\textit{\scriptsize neg}}$ & Peso para rotação negativa da roda\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

Cada neurônio tem quatro entradas. A função $f$ deve ser não-linear a fim de limitar as velocidades máximas para frente e para trás. O ponto grande na parte de trás do robô denota uma entrada constante de $1$ que é ponderada por $w_{\textit{\scriptsize fwd}}$. Isto assegura que, na ausência de sinais dos sensores, o robô irá avançar. Ao implementar a ANN você precisa encontrar um peso para que as potências do motor de saída sejam razoáveis na ausência de entrada dos sensores. O peso também deve garantir que a entrada constante seja semelhante às entradas dos sensores.

Os valores $x_1, x_2, x_3$ vêm dos sensores que retornam zero quando não há nenhum objeto e um valor positivo crescente ao se aproximar de um objeto. O sensor central é conectado aos dois neurônios com peso negativo $-w_{\textit{\scriptsize back}}$ para que se for detectado um obstáculo, o robô se mova para trás. Este peso deve ser ajustado a um valor que faça o robô recuar lentamente.

Os sensores esquerdo e direito são conectados aos neurônios com um peso positivo para o neurônio que controla a roda próxima e um peso negativo para o neurônio que controla a roda distante. Isto assegura que o robô se afaste do obstáculo.

The following activity asks you to think about the relative values of weights.

\begin{framed}
\act{ANN para evitar obstáculos: design}{avoidance-design}
\begin{itemize}
\item Que relação deve haver entre $w_{\textit{\scriptsize fwd}}$ e $w_{\textit{\scriptsize back}}$?
\item Que relação deve haver entre $w_{\textit{\scriptsize fwd}}$ e $w_{\textit{\scriptsize pos}}$ and between $w_{\textit{\scriptsize fwd}}$ e $w_{\textit{\scriptsize neg}}$?
\item Que relação deve haver entre $w_{\textit{\scriptsize back}}$ e $w_{\textit{\scriptsize pos}}$ e entre $w_{\textit{\scriptsize back}}$ e  $w_{\textit{\scriptsize neg}}$?
\item Que relação deve haver entre $w_{\textit{\scriptsize pos}}$ e $w_{\textit{\scriptsize neg}}$?
\item O que acontece se o obstáculo for detectado tanto pelo sensor esquerdo como pelo sensor central?
\end{itemize}
\end{framed}

Nas atividades seguintes, você terá que experimentar os pesos e as funções para atingir o comportamento desejado. Seu programa deve usar uma estrutura de dados como uma matriz para que seja fácil alterar os valores dos pesos.

\begin{framed}
\act{ANN para evitar obstáculos: implementação}{avoidance-implementation}
\begin{itemize}
\item Escreva um programa para evitar obstáculos usando a ANN na Fig.~ \ref{fig.nn-avoidance}.
\end{itemize}
\end{framed}

\begin{framed}
\act{ANN para atração de obstáculos}{object-attraction}
\begin{itemize}
\item Escreva um programa para implementar a atração de obstáculos usando uma ANN:
\begin{itemize}
\item O robô move-se para a frente.
\item Se o sensor central detectar que o robô está \emph{muito próximo} do obstáculo, ele pára.
\item Se um obstáculo for detectado pelo sensor esquerdo, o robô vira à esquerda.
\item Se um obstáculo for detectado pelo sensor direito, o robô vira à direita.
\end{itemize}
\end{itemize}
\end{framed}

\section{Redes neurais artificiais: topologias}\label{s.ann-topology}

O exemplo na seção anterior é baseado em uma rede neural artificial composta de uma única camada de dois neurônios, cada um com várias entradas e uma única saída. Esta é uma topologia muito simples para uma rede neural artificial; muitas outras topologias podem implementar algoritmos mais complexos (Fig.~\ref{fig.nn-deep}). Atualmente, as ANNs com milhares ou mesmo milhões de neurônios dispostos em muitas camadas são usadas para implementar \emph{deep learning}. Nesta seção apresentamos uma visão geral de algumas topologias de ANNs.

\begin{figure}
\begin{center}
% Deep network
\begin{tikzpicture}%
[node distance=1.5cm and 1.5cm, nn/.style={circle,draw,minimum size=16pt}]
% Inputs
\node (i1) {};
\node (i2) [below=of i1] {};
\node (i3) [below=of i2] {};
\foreach \i/\up in {i1/5pt,i2/4pt,i3/4pt}
  \draw (\i) [yshift=-\up] arc[start angle=-90, end angle=90, radius=4pt] {};
% Nodes
\foreach \old/\new in {
  i1/n1, i2/n2, i3/n3, n1/n4, n2/n5, n3/n6,
  n4/n7, n5/n8, n6/n9, n7/n10, n8/n11, n9/n12}
    \node (\new) [nn] [right=of \old] {};
% Outputs
\node (o1) [right=of n10] {};
\node (o2) [right=of n11] {};
\node (o3) [right=of n12] {};
% Arrows from input and to output
\foreach \source/\target in {i1/n1, i2/n2, i3/n3, n10/o1, n11/o2, n12/o3}
  \draw[->] (\source) -- (\target);
% Arrows from first set of nodes
\foreach \source in {n1, n2, n3} {
  \foreach \target in {n4, n5, n6} {
    \draw[->] (\source) -- (\target);
  }
}
% Ellipsis
\foreach \source/\target in {n4/n7, n5/n8, n6/n9}
  \path (\source) -- node {$\cdots$} (\target);
% Arrows to second set of nodes
\foreach \source in {n7, n8, n9} {
  \foreach \target in {n10, n11, n12} {
    \draw[->] (\source) -- (\target);
  }
}
\end{tikzpicture}
\caption{Rede neural para o aprendizado profundo}\label{fig.nn-deep}
\end{center}
\end{figure}


\subsection{Topologia multicamadas}

\begin{figure}
\begin{minipage}{.5\textwidth}
% Multilayer ANN
\begin{tikzpicture}%
[scale=.7,node distance=1cm and 1.5cm, nn/.style={circle,draw,minimum size=16pt}]
% Inputs
\node (i1) {};
\node (i2) [below=of i1] {};
\node (i3) [below=of i2] {};
\node (i4) [below=of i3] {};
\foreach \i/\up in {i1/5pt,i2/4pt,i3/4pt,i4/4pt}
  \draw (\i) [yshift=-\up] arc[start angle=-90, end angle=90, radius=4pt] {};
 Nodes
\foreach \old/\new in {i1/n1, i2/n2, i3/n3, i4/n4, n2/n5, n3/n6}
    \node (\new) [nn] [right=of \old] {};
% Outputs
\node (o1) [right=of n5] {};
\node (o2) [right=of n6] {};
% Arrows from input
\foreach \source in {i1, i2, i3, i4} {
  \foreach \target in {n1, n2, n3, n4} {
    \draw[->] (\source) -- (\target);
  }
}
% Arrows from nodes to nodes
\foreach \source in {n1, n2, n3, n4} {
  \foreach \target in {n5, n6} {
    \draw[->] (\source) -- (\target);
  }
}
% Arrows to output
\draw[->] (n5) -- (o1);
\draw[->] (n6) -- (o2);
\end{tikzpicture}
\caption{Multilayer ANN}\label{fig.multi-ann}
\end{minipage}
\hspace{\fill}
% ANN with memory
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}%
[baseline=-40mm,node distance=1cm and 1.5cm, nn/.style={circle,draw,minimum size=16pt}]
% Inputs
\node (i1) {};
\node (i2) [below=of i1,yshift=-2pt] {};
\node (i3) [below=of i2,yshift=-2pt] {};
\foreach \i/\up in {i1/5pt,i2/4pt,i3/4pt}
  \draw (\i) [yshift=-\up,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
% Nodes
\node (n1) [nn] [right=of i1,yshift=-13mm] {};
\node (n2) [nn] [right=of i2,yshift=-13mm] {};
% Outputs
\node (o1) [right=of n1] {};
\node (o2) [right=of n2] {};
% Left-to-right arrows
\foreach \source in {i1, i2, i3} {
  \foreach \target in {n1, n2} {
    \draw[->] (\source) -- (\target);
  }
}
% Diagonal arrows and self-loops
\draw[->] (n1) -- (o1);
\draw[->] (n2) -- (o2);
\path[<-] (n1) edge[loop above, min distance=15mm, out=120, in=40] (n1);
\path[->] (n2) edge[loop below, min distance=15mm, out=-40, in=-120] (n2);
\end{tikzpicture}
\caption{ANN com memória}\label{fig.ann-memory}
\end{minipage}
\end{figure}

%\begin{figure}
%\subfigures
%\begin{minipage}{\textwidth}
%% Multilayer ANN
%\leftfigure{
%\begin{tikzpicture}%
%[scale=.7,node distance=1cm and 1.5cm, nn/.style={circle,draw,minimum size=16pt}]
%% Inputs
%\node (i1) {};
%\node (i2) [below=of i1] {};
%\node (i3) [below=of i2] {};
%\node (i4) [below=of i3] {};
%\foreach \i/\up in {i1/5pt,i2/4pt,i3/4pt,i4/4pt}
%  \draw (\i) [yshift=-\up] arc[start angle=-90, end angle=90, radius=4pt] {};
% Nodes
%\foreach \old/\new in {i1/n1, i2/n2, i3/n3, i4/n4, n2/n5, n3/n6}
%    \node (\new) [nn] [right=of \old] {};
%% Outputs
%\node (o1) [right=of n5] {};
%\node (o2) [right=of n6] {};
%% Arrows from input
%\foreach \source in {i1, i2, i3, i4} {
%  \foreach \target in {n1, n2, n3, n4} {
%    \draw[->] (\source) -- (\target);
%  }
%}
%% Arrows from nodes to nodes
%\foreach \source in {n1, n2, n3, n4} {
%  \foreach \target in {n5, n6} {
%    \draw[->] (\source) -- (\target);
%  }
%}
%% Arrows to output
%\draw[->] (n5) -- (o1);
%\draw[->] (n6) -- (o2);
%\end{tikzpicture}
%}
%\hspace{\fill}
%% ANN with memory
%\rightfigure{
%\begin{tikzpicture}%
%[baseline=-40mm,node distance=1cm and 1.5cm, nn/.style={circle,draw,minimum size=16pt}]
%% Inputs
%\node (i1) {};
%\node (i2) [below=of i1,yshift=-2pt] {};
%\node (i3) [below=of i2,yshift=-2pt] {};
%\foreach \i/\up in {i1/5pt,i2/4pt,i3/4pt}
%  \draw (\i) [yshift=-\up,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
%% Nodes
%\node (n1) [nn] [right=of i1,yshift=-13mm] {};
%\node (n2) [nn] [right=of i2,yshift=-13mm] {};
%% Outputs
%\node (o1) [right=of n1] {};
%\node (o2) [right=of n2] {};
%% Left-to-right arrows
%\foreach \source in {i1, i2, i3} {
%  \foreach \target in {n1, n2} {
%    \draw[->] (\source) -- (\target);
%  }
%}
%% Diagonal arrows and self-loops
%\draw[->] (n1) -- (o1);
%\draw[->] (n2) -- (o2);
%\path[<-] (n1) edge[loop above, min distance=15mm, out=120, in=40] (n1);
%\path[->] (n2) edge[loop below, min distance=15mm, out=-40, in=-120] (n2);
%\end{tikzpicture}
%}
%\leftcaption{Multilayer ANN}\label{fig.multi-ann}
%\rightcaption{ANN with memory}\label{fig.ann-memory}
%\end{minipage}
%\end{figure}

A figura~\ref{fig.multi-ann} mostra um ANN com várias camadas de neurônios. As camadas adicionais podem implementar cálculos mais complexos do que uma única camada. Por exemplo, com uma única camada não é possível fazer o robô avançar quando apenas um sensor detecta um obstáculo e recuar quando vários sensores detectam um obstáculo. A razão é que a função em uma única camada ligando os sensores e os motores é monotônica, ou seja, pode fazer com que o motor vá mais rápido quando a entrada do sensor aumenta ou mais lento quando a entrada do sensor aumenta, mas não os dois. A camada de neurônios conectada à saída é chamada de \emph{output layer}, enquanto as camadas internas são chamadas de \emph{hidden layers}.


\begin{framed}
\act{Multilayer ANNs}{multilayer-understanding}
\begin{itemize}
\item O objetivo desta atividade é entender como as ANNs de várias camadas podem realizar cálculos que uma ANN de uma única camada não pode realizar. Para a atividade, assumir que as entradas $x_i$ estão na faixa de $-2,0$ a $2,0$, os pesos $w_i$ estão na faixa de $-1,0$ a $1,0$, e as funções $f$ limitam os valores de saída à faixa de $-1,0$ a $1,0$.
\item Para o ANN que consiste de um único neurônio (Fig.~\ref{fig.artificial-neuron-1}) com $w_1=-0,5$, calcular $y_1$ para entradas em incrementos de $0,2$: $x_1=-2,0, -1,8, \ldots, 0,0, \ldots, 1,8, 2,0$. Traçar os resultados em um gráfico.
\item Repetir o cálculo para vários valores de $w_1$. O que você pode dizer sobre a relação entre a saída e a entrada?
\item Considere o ANN de duas camadas mostrado na Fig.~\ref{fig.two-layer-ann} com pesos:
\[
 w_{11}=1,\, w_{12}=0.5,\, w_{21}=1,\, w_{22}=-1\,. 
\]
Calcular os valores e desenhar gráficos das saídas dos neurônios da camada oculta (os neurônios da esquerda) e da camada de saída (o neurônio da direita). Você pode obter a mesma saída de um ANN com apenas uma camada?
\end{itemize}
\end{framed}
\begin{framed}

\act{ANN de várias camadas para evitar obstáculos}{multilayer-avoidance}
\begin{itemize}
\item Projetar uma ANN que implemente o seguinte comportamento de um robô: Existem dois sensores frontais. Quando um objeto é detectado na frente de um dos sensores, o robô gira para evitar o objeto, mas quando um objeto é detectado por ambos os sensores, o robô se move para trás.
\end{itemize}
\end{framed}

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm and 2.5cm, on grid]
\pic { nnnode={nn1} };
\pic [right=of nn1] { nnnode={nn2} };
\pic [below=of nn1] { nnnode={nn3} };
\node (output) [right=of nn2] {};
\node (input) [left=of nn1] {};
\draw (input) [yshift=-4pt,xshift=-1pt] arc[start angle=-90, end angle=90, radius=4pt] {};
\draw[->] (input.east) -- node[w] { $w_{11}$ } (nn1.west);
\draw[->] (nn1.east) -- node[w] { $w_{21}$ } (nn2.west);
\draw[->] (nn2.east) -- (output.west);
\draw[->] (input.east) -- node[w,yshift=3pt] { $w_{12}$ } (nn3.west);
\draw[->] (nn3.east) -- node[w,yshift=-3pt] { $w_{22}$ } (nn2.south west);
\end{tikzpicture}
\caption{Duas camadas ANN}\label{fig.two-layer-ann}
\end{center}
\end{figure}

\subsection{Memória}\label{s.ann-memory}

Uma rede neural artificial pode ter \emph{conexões recorrentes} desde a saída de um neurônio até a entrada de um neurônio na mesma camada (inclusive ela mesma). Conexões recorrentes podem ser usadas para implementar a memória. Considere o veículo Braitenberg para evitar obstáculos (Fig.~\ref{fig.nn-avoidance}). Ele só gira quando os obstáculos são detectados pelos sensores. Quando eles não são mais detectados, o robô não continua a girar. Adicionando conexões recorrentes, podemos introduzir um efeito de memória que faz o robô continuar girando. Suponha que cada um dos sensores cause uma entrada de $0,75$ para os neurônios e que a saída seja saturada até $1,0$ pela função de saída não-linear. Se os sensores não detectarem mais o obstáculo, as entradas se tornam $0$, mas a conexão recorrente adiciona uma entrada de $1,0$ para que a saída permaneça $1,0$.

\begin{framed}
\act{ANN com memória}{memory}
\begin{itemize}
\item Considere a rede na Fig.~ref{fig.ann-memory} com uma função de saída que se satura a $0$ e $1$. As entradas e a maioria dos pesos também estão entre $0$ e $1$. O que acontece se o peso das conexões recorrentes na figura for maior que $1$? O que acontece se o peso estiver entre $0$ e $1$?
\item Modificar a implementação da rede na Fig.~ref{fig.nn-avoidance} para adicionar conexões recorrentes nos dois neurônios de saída. Qual é o efeito deles no comportamento de obstáculo-evasão do robô?
\end{itemize}
\end{framed}

\subsection{Filtro espacial}

Uma câmera é um dispositivo de detecção construído a partir de um grande número de sensores adjacentes (um para cada pixel). Os valores dos sensores podem ser as entradas para uma ANN com um grande número de neurônios na primeira camada (Fig.~\ref{fig.ann-spatial-filtering}). Pixels próximos serão introduzidos aos neurônios adjacentes. A rede pode ser usada para extrair características locais tais como diferenças de intensidade entre pixels adjacentes em uma imagem, e esta propriedade local pode ser usada para tarefas como identificar bordas na imagem. O número de camadas pode ser uma ou mais. Esta topologia de neurônios é chamada de \emph{filtro espacial} porque pode ser usada como um filtro antes de uma camada que implementa um algoritmo para evitar obstáculos.

\begin{figure}
\begin{center}
\begin{tikzpicture}%
[node distance=1.5cm and 2cm, nn/.style={circle,draw,minimum size=16pt}]
% Inputs
\node (i1) {};
\node (i2) [below=of i1] {};
\node (i3) [below=of i2] {};
\node (i4) [below=of i3] {};
\node (i5) [below=of i4] {};
\foreach \i in {i1,i2,i3,i4,i5}
  \draw (\i) [yshift=-4pt] arc[start angle=-90, end angle=90, radius=4pt] {};
% Nodes
\foreach \old/\new in {i1/n1, i2/n2, i3/n3, i4/n4, i5/n5}
    \node (\new) [nn] [right=of \old] {};
% Outputs
\foreach \old/\new in {n1/o1, n2/o2, n3/o3, n4/o4, n5/o5}
    \node (\new) [node distance=1.5cm and 1cm,right=of \old] {};
% Arrows to output
\foreach \source/\target in {n1/o1/, n2/o2, n3/o3, n4/o4/, n5/o5}
    \draw[->] (\source) -- (\target);
% Arrows from input (right)
\foreach \source/\target/\weight in {
      i1/n1/$+4$, i2/n2/$+4$, i3/n3/$+4$, i4/n4/$+4$, i5/n5/$+4$}
    \draw[->] (\source) -- node[near start,fill=white,inner sep=1pt] {\weight} (\target);
% Arrows from input (up, down)
\foreach \source/\target/\weight in {
  i1/n2/$-2$, i2/n1/$-4$, i2/n3/$-2$, i3/n2/$-2$, i3/n4/$-2$,
  i4/n3/$-2$, i5/n4/$-2$, i4/n5/$-4$}
    \draw[->] (\source) -- node[near end,fill=white,inner sep=1pt] {\weight} (\target);
\end{tikzpicture}
\caption{ANN para filtragem espacial}\label{fig.ann-spatial-filtering}
\end{center}
\end{figure}

\noindent\textbf{Exemplo} A ANN na Fig.~\ref{fig.ann-spatial-filtering} pode ser usada para distinguir entre objetos estreitos e largos. Por exemplo, tanto uma perna de uma cadeira quanto uma parede são detectados como objetos, mas o primeiro é um obstáculo que pode ser evitado por uma seqüência de curvas, enquanto que uma parede não pode ser evitada, então o robô deve dar a volta ou seguir a parede.

Suponha que a perna da cadeira seja detectada pelo sensor do meio com um valor de $60$, mas como a perna é estreita os outros sensores retornam o valor de $0$. Os valores de saída da ANN (de cima para baixo) são:
\begin{eqnarray*}
(0\times 4) \,+ \, (0\times -4) &=& 0\\
(0\times -2) \,+ \, (0\times 4) \,+ \, (60\times -2)&=&-120\\
(0\times -2) \,+ \, (60\times 4) \,+ \,  (0\times -2)&=&+240\\
(60\times -2) \,+ \, (0\times 4) \,+ \, (0\times -2)&=&-120\\
(0\times 4) \,+ \, (0\times -4) &=&0\,.
\end{eqnarray*}
Quando o robô se aproxima de uma parede, todos os sensores retornarão mais ou menos os mesmos valores, digamos, $45, 50, 40, 55, 50$. Os valores de saída da ANN são:
\begin{eqnarray*}
(45\times 4)  \,+ \, (50\times -4)&=&-20\\
(45\times -2) \,+ \, (50\times 4) \,+ \, (40\times -2)&=&+30\\
(50\times -2) \,+ \, (40\times 4) \,+ \, (55\times -2)&=&-50\\
(40\times -2) \,+ \, (55\times 4) \,+ \, (50\times -2)&=&+40\\
(55\times -4) \,+ \, (50\times 4)&=&-20\,.
\end{eqnarray*}
Embora $48$, o valor médio retornado pelos sensores que detectam a parede, seja aproximadamente o mesmo que o valor $60$ retornado ao detectar a perna da cadeira, as saídas das ANNs são claramente distinguíveis. O primeiro conjunto de valores tem um alto valor de pico cercado por vizinhos com grandes valores negativos, enquanto o segundo conjunto é um conjunto relativamente plano de valores na faixa de $-50$ a $40$. A camada de neurônios pode identificar com segurança se o objeto é estreito ou largo.

\begin{framed}
\act{ANN para filtragem espacial}{spatial}
\begin{itemize}
\item Implementar a ANN para filtragem espacial em Fig.~\ref{fig.ann-spatial-filtering}.
\item As entradas para o ANN são as leituras de cinco sensores de proximidade voltados para a frente. Se apenas um sensor detecta um objeto, o robô vira-se para enfrentar o objeto. Se o sensor central for aquele que detecta o objeto, o robô se move para a frente. 
\item Implementar três comportamentos do robô quando ele detectar uma parede definida como os cinco sensores que detectam um objeto:
\begin{itemize}
\item O robô pára.
\item O robô se move para a frente.
\item O robô se move para trás.
\end{itemize}
Lembre-se de que não há \p(se) declarações em uma rede neural artificial; você só pode adicionar neurônios adicionais ou alterar os pesos associados às entradas dos neurônios. Observe novamente a Activity~\ref{act.multilayer-avoidance} que usou dois níveis de neurônios para implementar um comportamento semelhante.
\item A implementação envolverá a adição de dois neurônios adicionais cujas entradas são as saídas da primeira camada. A saída do primeiro neurônio ajustará a potência do motor esquerdo e a saída do segundo neurônio ajustará a potência do motor direito.
\item O que acontece se um objeto for detectado por dois sensores adjacentes?
\end{itemize}
\end{framed}

\section{Aprendizagem}\label{s.ann-learning}

Ajustar os pesos manualmente é difícil mesmo para redes muito pequenas, como as apresentadas nas seções anteriores. Em organismos biológicos, as sinapses têm uma plasticidade que permite \emph{aprendizagem}. O poder das redes neurais artificiais vem de sua capacidade de aprender, ao contrário dos algoritmos comuns que têm que ser especificados até o último detalhe. Há muitas técnicas de aprendizagem nas ANNs; descrevemos uma das técnicas mais simples nesta seção e mostramos como ela pode ser usada na rede neural para evitar obstáculos.

\subsection{Categorias de algoritmos de aprendizagem}

Há três categorias principais de algoritmos de aprendizagem:

\begin{itemize}
\item \textbf{O aprendizado supervisionado} é aplicável quando sabemos qual é o resultado esperado para um conjunto de entradas. O erro entre a saída desejada e a real é usado para corrigir os pesos, a fim de reduzir o erro. Por que é necessário treinar uma rede se já sabemos como ela deve se comportar? Uma razão é que a rede é obrigada a fornecer saídas em situações para as quais não foi treinada. Se os pesos forem ajustados para que a rede se comporte corretamente em entradas conhecidas, é razoável supor que seu comportamento será mais ou menos correto em outras entradas. Uma segunda razão para treinar uma rede é simplificar o processo de aprendizagem: em vez de relacionar diretamente as saídas $\{y_i\}$ com valores específicos das entradas $\{x_i\}$, é mais fácil colocar a rede em várias situações diferentes e dizer-lhe quais saídas são esperadas em cada situação.

\item No \textbf{reinforcement learning} não especificamos o valor exato da saída em cada situação; em vez disso, simplesmente dizemos à rede se a saída que ela calcula é boa ou não. O reforço é apropriado quando podemos facilmente distinguir o comportamento correto do comportamento incorreto, mas realmente não nos importamos qual é a saída exata para cada situação. Na próxima seção, apresentamos o aprendizado de reforço para evitar obstáculos por um robô; para esta tarefa é suficiente que o robô evite o obstáculo e não nos importamos com os ajustes do motor de saída da rede, desde que o comportamento esteja correto.

\item \emph{aprendizagem sem supervisão} é a aprendizagem sem feedback externo, onde a rede se adapta a um grande número de entradas. O aprendizado sem supervisão não é apropriado para atingir objetivos específicos; em vez disso, é usado em problemas de classificação onde a rede é apresentada com dados brutos e tentativas de encontrar tendências dentro dos dados. Esta abordagem de aprendizagem é o tópico do Chap.~\ref{ch.machine}.
\end{itemize}

\subsection{A regra Hebbian para a aprendizagem nas ANNs}\label{s.hebbian-rule}

A \emph{Hebbian rule} é uma técnica de aprendizagem simples para as ANNs. É uma forma de aprendizagem de reforço que modifica os pesos das conexões entre os neurônios. Quando a rede está fazendo algo bom, reforçamos esta boa resposta: se o valor de saída de dois neurônios conectados é semelhante, aumentamos o peso da conexão que os liga, enquanto que se eles são diferentes, diminuímos o peso. Se o robô estiver fazendo algo errado, podemos ou diminuir o peso de neurônios conectados similares, ou não fazer nada.

A mudança no peso da conexão entre neurônio $k$ e neurônio $j$ é descrita pela equação:
\[
\Delta w_{kj}\,=\,\alpha \, y_{k} \, x_{j}\,,\label{eq.hebbian}
\]
onde $w_{kj}$ é o peso que liga os neurônios $k$ e $j$, $Delta w_{kj}$ é a mudança de $w_{kj}$, $y_{k}$ é a saída do neurônio $k$ e $x_{j}$ a entrada do neurônio $j$, e $\alpha$ é uma constante que define a velocidade do aprendizado. 

A regra Hebbian é aplicável sob duas condições:
\begin{itemize}
\item O robô está explorando seu ambiente, encontrando várias situações, cada uma com suas próprias entradas para as quais a rede computa um conjunto de saídas.
\item O robô recebe informações sobre quais comportamentos são bons e quais não são.
\end{itemize}
A avaliação da qualidade do comportamento do robô pode vir de um observador humano dando feedback manualmente; alternativamente, um sistema automático pode ser usado para avaliar o comportamento. Por exemplo, para ensinar o robô a evitar obstáculos, uma câmera externa pode ser usada para observar o robô e avaliar seu comportamento. O comportamento é classificado como ruim quando o robô se aproxima de um obstáculo e como bom quando o robô está se afastando de todos os obstáculos. É importante entender que o que é avaliado não é o \emph{state} do robô (perto ou longe de um obstáculo), mas sim o \emph{comportamento} do robô (aproximando-se ou evitando um obstáculo). A razão é que as conexões da rede neural geram um comportamento baseado no estado medido pelos sensores.

\subsubsection*{Aprendendo a evitar um obstáculo}

Suponhamos que queremos ensinar um robô a evitar um obstáculo. Uma maneira seria deixar o robô mover-se aleatoriamente no ambiente, e depois tocar uma tecla quando ele evita o obstáculo com sucesso e outra quando ele se choca com o obstáculo. O problema com esta abordagem é que provavelmente levará muito tempo para o robô exibir um comportamento que pode ser definitivamente caracterizado como positivo (evitando o obstáculo) ou negativo (chocando-o contra o obstáculo).

Alternativamente, podemos apresentar ao robô várias situações conhecidas e o comportamento exigido: (1) detectar um obstáculo à esquerda e virar à direita é bom; (2) detectar um obstáculo à direita e virar à esquerda é bom; (3) detectar um obstáculo à frente e virar para trás é bom; (4) detectar um obstáculo à frente e virar à frente é mau.

Isto parece aprendizado supervisionado, mas não é, porque o feedback para o robô é usado apenas para reforçar os pesos ligados ao bom comportamento. O aprendizado supervisionado consistiria em quantificar o erro entre as saídas desejadas e reais (para os motores, neste caso), e usar este erro para ajustar os pesos para calcular as saídas exatas. O feedback no aprendizado do reforço é binário: o comportamento é bom ou não.

\begin{figure}
\begin{center}
\begin{tikzpicture}
% Draw big robot
\draw (-2.2cm,-3.5cm) to [rounded corners] (5cm,-3.5cm) to%
[rounded corners, bend right=45] (5cm,3.5cm) to (-2.2cm,3.5cm) to cycle;
\fill (-1.2cm,-3.5cm) rectangle +(2.5cm, -.4cm);
\fill (-1.2cm,3.5cm) rectangle +(2.5cm, .4cm);
% Draw two sum nodes and arrows to wheels
\node[draw,circle] at (0,14mm) (left) {$+$};
\node[draw,circle] at (0,-14mm) (right) {$+$};
\draw[->,thick] (left) -- node[fill=white] {$y_1$} (0,34mm);
\draw[->,thick] (right) -- node[fill=white] {$y_2$} (0,-34mm);
% Left sensor and arrows
\draw[red] (5, 3) arc[start angle=135, end angle=315, radius=.2cm];
\node at (5.2,2.9) {$x_1$};
\draw[->,thick,red] (4.95,2.8) -- node[w] {$w_{1l}$} (left.north east);
\draw[->,thick,red] (5,2.7) -- node[w,near start,xshift=6pt] {$w_{1r}$} (right.north east);
% Center sensor and arrows
\draw[green!60!black] (6, .2) arc[start angle=90, end angle=270, radius=.2cm];
\node at (6.2,0) {$x_2$};
\draw[->,thick,green!60!black] (5.8,.1) -- node[w] {$w_{2l}$} (left.east);
\draw[->,thick,green!60!black] (5.8,-.1) -- node[w] {$w_{2r}$} (right.east);
% Right sensor and arrows
\draw[blue] (5.3, -2.8) arc[start angle=45, end angle=225, radius=.2cm];
\node at (5.2,-2.95) {$x_3$};
\draw[->,thick,blue] (5,-2.8) -- node[w,near start,xshift=6pt] {{$w_{3l}$}} (left.south east);
\draw[->,thick,blue] (4.95,-2.9) -- node[w] {$w_{3r}$} (right.south east);
% Left rear sensor and arrows
\draw (-1.9, 12mm) arc[start angle=-90, end angle=90, radius=.2cm];
\node at (-1.98,14mm) {$x_4$};
\draw[->] (-1.7,14mm) -- node[fill=white] {$w_{4l}$} (left.west);
\draw[->] (-1.7,13.5mm) -- node[fill=white,near start] {$w_{4r}$} (right.west);
% Right rear sensor and arrows
\draw (-1.9, -16mm) arc[start angle=-90, end angle=90, radius=.2cm];
\node at (-1.98,-14mm) {$x_5$};
\draw[->,thick] (-1.7,-13.5mm) -- node[fill=white,near start] {$w_{5l}$} (left.west);
\draw[->,thick] (-1.7,-14mm) -- node[fill=white] {$w_{5r}$} (right.west);
\end{tikzpicture}
\end{center}
\caption{Rede neural para demonstrar o aprendizado Hebbiano}\label{fig.hebbian-avoidance}
\end{figure}

\subsubsection*{O algoritmo para evitar obstáculos}

Vamos agora demonstrar a regra Hebbian para aprender sobre o problema de evitar obstáculos. Fig.~\ref{fig.hebbian-avoidance} é semelhante à Fig.~\ref{fig.nn-avoidance} exceto que sensores de proximidade foram adicionados à parte de trás do robô. Também mudamos a notação dos pesos para torná-los mais apropriados para expressar a regra hebbian; em particular, os sinais negativos foram absorvidos pelos pesos.

O algoritmo de prevenção de obstáculos é implementado usando vários processos simultâneos e será exibido como um conjunto de três algoritmos. Algoritmo~\ref{alg.hebb-background} implementa um ANN que lê as entradas dos sensores e calcula as saídas para os motores. Os números de entradas e saídas são tirados da Fig.~\ref{fig.hebbian-avoidance}. Algoritmo~\ref{alg.hebb-feedback} recebe avaliações do comportamento do robô de um humano. O Algorithm~\ref{alg.hebb-rule} executa os cálculos da regra Hebbian para aprendizagem.

No Algorithm~\ref{alg.hebb-background} um cronômetro é definido para um período como $100$ milissegundos. O temporizador é decretado pelo sistema operacional (não mostrado) e quando expira, as saídas $y_1$ e $y_2$ são computadas pelo Eq.~\ref{eqn.hebbian} abaixo. Estas saídas são então utilizadas para ajustar a potência dos motores esquerdo e direito e, finalmente, o timer é reinicializado.

\begin{figure}
\begin{alg}{ANN para evitar obstáculos}{hebb-background}
&\idv{}integer period \ass $\cdots$& // Timer period (ms)\\
&\idv{}integer timer \ass period&\\
&\idv{}float array[5] \ \ \ \ \ $\vec{x}$& \\
&\idv{}float array[2] \ \ \ \ \ $\vec{y}$& \\
&\idv{}float array[2,5] \ \ $\vec{W}$&\\
\hline
\stl{}&when timer expires&\\
\stl{}&\idc{}$\vec{x}$ \ass sensor values&\\
\stl{}&\idc{}$\vec{y}$ \ass $\vec{W}\;\vec{x}$&\\
\stl{}&\idc{}left-motor-power \ass $\vec{y}$[1]&\\
\stl{}&\idc{}right-motor-power \ass $\vec{y}$[2]&\\
\stl{}&\idc{}timer \ass period&\\
\end{alg}
\end{figure}

Há cinco sensores que são lidos nas cinco variáveis de entrada:
\begin{center}
\begin{tabular}{lcl}
$x_1$ & $\leftarrow$ & \p{front left sensor}\\
$x_2$ & $\leftarrow$ & \p{front center sensor}\\
$x_3$ & $\leftarrow$ & \p{front right sensor}\\
$x_4$ & $\leftarrow$ & \p{rear left sensor}\\
$x_5$ & $\leftarrow$ & \p{rear right sensor}
\end{tabular}
\end{center}

Supomos que os valores dos sensores estejam entre $0$ (obstáculo não detectado) e $100$ (obstáculo muito próximo), e que os valores das potências dos motores estejam entre $-100$ (potência total para trás) e $100$ (potência total para frente). Se o cálculo resultar em saturação, os valores são truncados até os pontos finais da faixa, ou seja, um valor inferior a $-100$ torna-se $-100$ e um valor superior a $100$ torna-se $100$.\footnote{Algorithm~\ref{alg.hebb-background} declarou todas as variáveis como \textsf{\footnotesize float} porque os pesos são números de ponto flutuante. Se as entradas e saídas dos sensores forem inteiros, será necessária uma conversão do tipo.} Lembre-se de que um robô com acionamento diferencial gira à direita ajustando $y_1$ (a potência do motor esquerdo) para $100$ e $y_2$ (a potência do motor direito) para $-100$, e de forma semelhante para uma curva à esquerda.

Para simplificar a apresentação do Algoritmo~\ref{alg.hebb-background} usamos notação vetorial onde as entradas são dadas como um vetor de coluna única:
\[
\vec{x} = \left[ \begin{array}{c} x_1\\x_2\\x_3\\x_4\\x_5 \end{array} \right].
\]
Referindo-se novamente à Fig.~ref{fig.hebbian-avoidance}, o cálculo das saídas é dado por:
\begin{eqnarray}
y_1 & \leftarrow & w_{1l}x_1 + w_{2l}x_2 + w_{3l}x_3 + w_{4l}x_4 + w_{5l}x_5\label{eqn.yl}\\
y_2 & \leftarrow & w_{1r}x_1 + w_{2r}x_2 + w_{3r}x_3 + w_{4r}x_4 + w_{5r}x_5\label{eqn.yr}\,.
\end{eqnarray}
Expresso em notação vetorial, isto é:
\begin{equation}\label{eqn.hebbian}
\vec{y} =
\left[ \begin{array}{c} y_2\\y_2 \end{array} \right] =
\left[
  \begin{array}{ccccc}
    w_{1l} & w_{2l} & w_{3l} & w_{4l} & w_{5l} \\
    w_{1r} & w_{2r} & w_{3r} & w_{4r} & w_{5r} \\
\end{array}
\right]
\left[ \begin{array}{c} x_1\\x_2\\x_3\\x_4\\x_5 \end{array} \right] =
\vec{W}\;\vec{x}\,.
\end{equation}

Para que o algoritmo aprenda, o feedback é usado para modificar os pesos $\vec{W}$ (Algoritmos~\ref{alg.hebb-feedback}, {alg.hebb-rule}). Vamos supor que haja quatro botões no robô ou em um controle remoto, um para cada direção para frente, para trás, para a esquerda e para a direita. Sempre que notamos que o robô está em uma situação que requer um certo comportamento, tocamos no botão correspondente. Por exemplo, se o sensor esquerdo detectar um obstáculo, o robô deve virar à direita. Para implementar isto, há um processo para cada botão. Estes processos são mostrados juntos em Algorithm~\ref{alg.hebb-feedback}, onde os cortes para a frente \verb+/+ separam os eventos e ações correspondentes.

\begin{alg}{Feedback sobre o comportamento do robô}{hebb-feedback}
\hline
\stl{}&when button \{forward / backward / left / right\} touched &\\
\stl{}&\idc{}$y_{1}$ \ass \{$100$ / $-100$ / $-100$ / $100$\}&\\
\stl{}&\idc{}$y_{2}$ \ass \{$100$ / $-100$ / $100$ / $-100$\}&\\
\end{alg}

A próxima fase do algoritmo é atualizar os pesos de conexão de acordo com a regra Hebbian (Algorithm~\ref{alg.hebb-rule}).

\begin{alg}{Aplicando a regra Hebbiana}{hebb-rule}
\hline
\stl{}&\idc{}$\vec{x}$ \ass sensor values&\\
\stl{}&\idc{}for $j$ in $\{1,2,3,4,5\}$&\\
\stl{}&\idc{}\idc{}$w_{jl}$ \ass $w_{jl} + \alpha\, y_1\, x_j$&\\
\stl{}&\idc{}\idc{}$w_{jr}$ \ass $w_{jr} + \alpha\, y_2\, x_j$&\\
\end{alg}

\smallskip

\noindent\textbf{Exemplo} Assumir que inicialmente os pesos são todos iguais a zero. Por  Eqs.~\ref{eqn.yl}--\ref{eqn.yr} as saídas são zero e o robô não se moverá.

Suponha agora que um obstáculo é colocado na frente do sensor esquerdo de modo que $x_1=100$ enquanto $x_2=x_3=x_4=x_5=0$. Sem feedback nada acontecerá, já que os pesos ainda são zero. Se pressionarmos o botão direito (informando ao robô que o comportamento correto é virar à direita), as saídas são ajustadas para $y_1=100, y_2=-100$ para virar à direita, o que por sua vez leva às seguintes mudanças nos pesos (assumindo um fator de aprendizado $\alpha=0,0001$):
\begin{eqnarray*}
w_{1l} & \leftarrow & 0 + (0.0001 \times 100 \times 100) = 10\\
w_{1r} & \leftarrow & 0 + (0.0001 \times -100 \times 100) = -10\,.
\end{eqnarray*}
Da próxima vez que um obstáculo for detectado pelo sensor esquerdo, as saídas serão não-zero:
\begin{eqnarray*}
y_1 & \leftarrow & (10\times 100) + 0 + 0 + 0 + 0 = 1000\\
y_2 & \leftarrow & (-10\times 100) + 0 + 0 + 0 + 0 = -1000\,.\\
\end{eqnarray*}
Após a truncagem para $100$ e $-100$ estas saídas farão com que o robô vire à direita.

O fator de aprendizado $\alpha$ determina a magnitude do efeito de $y_k\, x_j$ nos valores de $w_{kj}$. Valores mais altos do fator causam efeitos maiores e, portanto, aprendizado mais rápido. Embora se possa pensar que um aprendizado mais rápido é sempre melhor, se o aprendizado for muito rápido pode causar mudanças indesejáveis, como esquecer situações anteriores boas ou enfatizar fortemente os erros. O fator de aprendizado deve ser ajustado para atingir o aprendizado ideal.

\begin{framed}
\act{Aprendizagem hebbiana para evitar obstáculos}{learn-avoid}
\begin{itemize}
\item Implementar Algorithms~\ref{alg.hebb-background}--\ref{alg.hebb-rule} e ensine seu robô a evitar obstáculos.
\item Modificar o programa de modo que ele se destaque para avançar quando não detectar um obstáculo.
\end{itemize}
\end{framed}

\section{Sumário}

Os robôs autônomos devem funcionar em ambientes caracterizados por um alto grau de incerteza. Por esse motivo, é difícil especificar algoritmos precisos para o comportamento do robô. As redes neurais artificiais podem implementar o comportamento requerido em um ambiente incerto aprendendo, ou seja, modificando e melhorando o algoritmo conforme o robô encontra situações adicionais. A estrutura das ANNs torna o aprendizado tecnicamente simples: Uma ANN é composta de um grande número de componentes pequenos e simples chamados neurônios e o aprendizado é alcançado pela modificação dos pesos atribuídos às conexões entre os neurônios.

O aprendizado pode ser supervisionado, reforçado ou não supervisionado. O aprendizado de reforço é apropriado para o aprendizado do comportamento robótico porque requer que o projetista especifique apenas se um comportamento observado é bom ou ruim, sem quantificar o comportamento. A regra Hebbian modifica os pesos que conectam os neurônios multiplicando a saída de um neurônio pela entrada do neurônio ao qual ele está conectado. O resultado é multiplicado por um fator de aprendizado que determina o tamanho da mudança no peso e, portanto, a taxa de aprendizado.

\section{Leitura adicional}

Haykin \cite{haykin} e Rojas \cite{rojas} são livros didáticos abrangentes sobre redes neurais. David Kriesel escreveu um tutorial online \cite{kriesel} que pode ser baixado gratuitamente.
